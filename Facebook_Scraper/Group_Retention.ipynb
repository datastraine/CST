{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Intialize](#Initalize)\n",
    "* [Scrapper](#Scraper)\n",
    "* [Clean Data](#Clean-Data)\n",
    "* [Store as CSV](#Store-as-CSV)\n",
    "* [Appendix](#Appendix)\n",
    "  * [Load Data from Roster Dictionary](#Load-From-Roster)\n",
    "  * [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalize\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import datetime as dt\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "from env import muser, mpass\n",
    "print('Init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapper\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code by pythonjar - grants browser permission \n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
    "chrome_options.add_experimental_option(\"prefs\",prefs)\n",
    "\n",
    "#specify the path to chromedriver.exe \n",
    "PATH = 'C:/Program Files (x86)/chromedriver.exe'\n",
    "#creates the driver object with the chrome driver and options as peramaters\n",
    "driver = webdriver.Chrome(PATH, options=chrome_options)\n",
    "\n",
    "#opens FB\n",
    "driver.get(\"http://www.facebook.com\")\n",
    "\n",
    "# waits for FB to load and display the login boxes using CSS_selector types\n",
    "username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='email']\")))\n",
    "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='pass']\")))\n",
    "\n",
    "#clears the user name and password fields and fills them with FB credentials\n",
    "username.clear()\n",
    "username.send_keys(muser)\n",
    "password.clear()\n",
    "password.send_keys(mpass)\n",
    "\n",
    "#waits 5 seconds or until the login button is displayed and then clicks it to login\n",
    "WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click() \n",
    "\n",
    "#wait 30 seconds to get security code for 2way auth\n",
    "time.sleep(30)\n",
    "\n",
    "# wait a random amount of time for the page to load before trying to navigate to the group members page\n",
    "# Note - if facebook limits this behavior, simulating clicks might be necessary\n",
    "time.sleep(random.randrange(4, 11))\n",
    "\n",
    "# create an empty list obj to store the list of dictionaries\n",
    "rosters = []\n",
    "\n",
    "# create a list of group member pages to pull rosters from replace the links inside the [] with the group you want to look at\n",
    "group = 'https://www.facebook.com/groups/tribunecommons/members/'\n",
    "\n",
    "\n",
    "# loop through the group urls\n",
    "driver.get(group)\n",
    "time.sleep(random.randrange(5, 10))\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# use a while true loop to scroll to the end of a group's member list before pulling the page_source\n",
    "while True:\n",
    "\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page a random amount of time between 6 and 12 seconds\n",
    "    time.sleep(random.randrange(6, 12))\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        # If heights are the same it will exit the function\n",
    "        break\n",
    "    last_height = new_height\n",
    "    \n",
    "    \n",
    "html = driver.page_source\n",
    "# create a soup object using html parser to use to prase the results\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "# create a list of all div cards that contains the information for each field we want to dump into a DF\n",
    "# Note the class values are likely to change, check it yourself.\n",
    "divs = soup.findAll('div', class_='ow4ym5g4 auili1gw rq0escxv j83agx80 buofh1pr g5gj957u i1fnvgqd oygrvhab cxmmr5t8 hcukyx3x kvgmc6g5 nnctdnn4 hpfvmrgz qt6c0cv9 jb3vyjys l9j0dhe7 du4w35lb bp9cbjyn btwxx1t3 dflh9lhu scb9dxdr')\n",
    "\n",
    "# Create a for loop that breaks into each div (data card) and find each instance of each piece of info \n",
    "# check each class label before running as Facebook changes these often\n",
    "for div in divs:\n",
    "    names = div.findAll('a', class_='oajrlxb2 g5ia77u1 qu0x051f esr5mh6w e9989ue4 r7d6kgcz rq0escxv nhd2j8a9 nc684nl6 p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso i1ao9s8h esuyzwwr f1sip0of lzcic4wl oo9gr5id gpro0wi8 lrazzd5p')\n",
    "\n",
    "    # Create empty objects to store the pulled text in the for loops for each piece of data\n",
    "    person = ''\n",
    "    date =  pd.to_datetime('today').date()\n",
    "\n",
    "    # loop through each div obj to pull the required information. This is required because the info is in list form\n",
    "    # some fields are blank hence doing if statements in for loops\n",
    "    for name in names:\n",
    "        person = name.text\n",
    "\n",
    "\n",
    "    # create a dictionary that contains the information for each member from each div obj\n",
    "    dicob = {'name':person,\n",
    "             'active':date}\n",
    "    # append the dictionary to the list\n",
    "    rosters.append(dicob)\n",
    "# extra sleep time before moving onto the next group\n",
    "time.sleep(8)\n",
    "\n",
    "# close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we get the dictionary into a DF, we need to create a list of people we don't want to grab. \n",
    "# This list will include all modmins and blocked people. \n",
    "# Note - this currently doesn't account for people with the same name but different facebook IDs. This can be fixed later\n",
    "\n",
    "ignore = ['Anthony Rivera Straine', 'Frank Straine', 'CrowdSourced Tribune', 'Mateo DeGall', \n",
    "         'Shawn Dixon', 'Jennifer P. Travis', 'Nikolette Adams', 'Lynn Paluga', 'Andrew Harris', \n",
    "         'David Marc Grant', 'Aaron Bratton', 'Peter William Essig', 'Vanessa Bibb-Cook', \n",
    "          'Ellen Grizwold', 'Ed Cummings', 'Kim Eckhoff', 'Nozofmary Ham']\n",
    "\n",
    "len(ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the roster list to a panda's data frame and drop any duplicate values\n",
    "retention = pd.DataFrame(rosters).drop_duplicates()\n",
    "# drop any row where the name = 'Learn More', 'YOUR NAME', or is blank (name = '')\n",
    "retention = retention[(retention['name'] != 'Learn More') & (retention['name'] != '') & (~retention['name'].isin(ignore))]\n",
    "# reset the index and drop the original index as it's values do not matter\n",
    "retention.reset_index(drop=True, inplace=True)\n",
    "retention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store as CSV\n",
    "\n",
    "This finalizes the recurit DF (for now). From here we can store the results as a useable CSV\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = pd.read_csv('Retention.csv')\n",
    "current.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention = pd.concat([current, retention]).reset_index(drop=True)\n",
    "retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention.to_csv('Retention.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Roster\n",
    "\n",
    "Use the cell below if you've already acquired the roster List and want to turn it into a CSV\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "### cell for recaputring dataframe from stored rosters ###\n",
    "##########################################################\n",
    "\n",
    "# convert the roster list to a panda's data frame and drop any duplicate whole row values\n",
    "recruit = pd.DataFrame(rosters).drop_duplicates()\n",
    "# drop any row where the name = 'Learn More', 'your name', or is blank (name = '')\n",
    "recruit = recruit[(recruit['name'] != 'Learn More') & (recruit['name'] != 'Anthony Rivera Straine') & (recruit['name'] != '')]\n",
    "# reset the index and drop the original index as it's values do not matter\n",
    "recruit.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "recruit.to_csv(\"Recruitment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load From CSV\n",
    "\n",
    "Use the cell below if you've already acquired the CSV and want to make changes to it\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "### cell for recaputring dataframe from stored csv ###\n",
    "######################################################\n",
    "\n",
    "current = pd.read_csv('Retention.csv')\n",
    "current.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "#current['retained'] = 1\n",
    "current = current.sort_values(by=[\"name\", \"active\"]).reset_index(drop=True)\n",
    "current['active'] = pd.to_datetime(current['active'])\n",
    "current['active'] = pd.to_datetime(current['active']).dt.to_period('M')\n",
    "current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type1 = current.groupby(['name'])[['active']].shift(1)\n",
    "type1.columns=(['previous_active'])\n",
    "type2 = current.groupby(['name'])[['active']].shift(-1)\n",
    "type2.columns=(['next_active'])\n",
    "analysis = pd.concat([current, type1], axis=1)\n",
    "analysis = pd.concat([analysis, type2], axis=1)\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[analysis['name']==\"Chris Soria\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pivotz = current.pivot(index='name', columns='active', values='retained')\n",
    "pivotz[(pivotz['2021-05-04'] == 1) & ((pivotz['2021-06-23'].isnull()) | (pivotz['2021-07-22'].isnull()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
