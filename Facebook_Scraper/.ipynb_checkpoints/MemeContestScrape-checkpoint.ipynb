{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Intialize](#Initalize)\n",
    "* [Scrapper](#Scraper)\n",
    "* [Clean Data](#Clean-Data)\n",
    "* [Store as CSV](#Store-as-CSV)\n",
    "* [Appendix](#Appendix)\n",
    "  * [Load Data from Roster Dictionary](#Load-From-Roster)\n",
    "  * [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalize\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "from env import suser, spass\n",
    "print('Init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapper\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code by pythonjar - grants browser permission \n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
    "chrome_options.add_experimental_option(\"prefs\",prefs)\n",
    "\n",
    "#specify the path to chromedriver.exe \n",
    "PATH = 'C:/Program Files (x86)/chromedriver.exe'\n",
    "#creates the driver object with the chrome driver and options as peramaters\n",
    "driver = webdriver.Chrome(PATH, options=chrome_options)\n",
    "\n",
    "#opens FB\n",
    "driver.get(\"http://www.facebook.com\")\n",
    "\n",
    "# waits for FB to load and display the login boxes using CSS_selector types\n",
    "username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='email']\")))\n",
    "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='pass']\")))\n",
    "\n",
    "#clears the user name and password fields and fills them with FB credentials\n",
    "username.clear()\n",
    "username.send_keys(suser)\n",
    "password.clear()\n",
    "password.send_keys(spass)\n",
    "\n",
    "#waits 5 seconds or until the login button is displayed and then clicks it to login\n",
    "WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click() \n",
    "\n",
    "# wait a random amount of time for the page to load before trying to navigate to the meme members page\n",
    "# Note - if facebook limits this behavior, simulating clicks might be necessary\n",
    "time.sleep(random.randrange(4, 11))\n",
    "\n",
    "# create an empty list obj to store the list of dictionaries\n",
    "rosters = []\n",
    "\n",
    "# create a meme obj to store the link of the meme contest page\n",
    "memes = 'https://www.facebook.com/groups/tribunecommons/posts/806085926758695/' # <--- replace value\n",
    "\n",
    "driver.get(memes)\n",
    "# wait 10 seconds for YOU to click on load more comments \n",
    "# Still trying to figure out how to do this automatically\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "html = driver.page_source\n",
    "# create a soup object using html parser to use to prase the results\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "# create a list of all div cards that contains the information for each field we want to dump into a DF\n",
    "# Note the class values are likely to change, check it yourself.\n",
    "divs = soup.findAll('div', class_='rj1gh0hx buofh1pr ni8dbmo4 stjgntxs hv4rvrfc')\n",
    "\n",
    "# Create a for loop that breaks into each div (data card) and find each instance of each piece of info \n",
    "# check each class label before running as Facebook changes these often\n",
    "for div in divs:\n",
    "    names = div.findAll('span', class_='d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d9wwppkn fe6kdd0r mau55g9w c8b282yb mdeji52x e9vueds3 j5wam9gi lrazzd5p oo9gr5id')\n",
    "    memelinks = div.findAll('a', class_='oajrlxb2 g5ia77u1 qu0x051f esr5mh6w e9989ue4 r7d6kgcz rq0escxv nhd2j8a9 nc684nl6 p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso i1ao9s8h esuyzwwr f1sip0of lzcic4wl')\n",
    "    reactions = div.findAll('span', class_='g0qnabr5 hyh9befq qt6c0cv9 n8tt0mok jb3vyjys j5wam9gi knj5qynh e9vueds3 m9osqain')\n",
    "\n",
    "\n",
    "    # Create empty objects to store the pulled text in the for loops for each piece of data\n",
    "    person = ''\n",
    "    votes = ''        \n",
    "    link = ''\n",
    "    # create a date column to get the month the contest took place in\n",
    "    date =  pd.to_datetime('today').date() + pd.tseries.offsets.DateOffset(months=-1)\n",
    " \n",
    "    # loop through each div obj to pull the required information. This is required because the info is in list form\n",
    "    # some fields are blank hence doing if statements in for loops\n",
    "    for name in names:\n",
    "        person = name.text\n",
    "    for meme in memelinks:\n",
    "        link = str(meme.get('href'))\n",
    "    for react in reactions:\n",
    "        votes = react.text\n",
    "\n",
    "    # create a dictionary that contains the information for each member from each div obj\n",
    "    dicob = {'name':person,\n",
    "             'link':link,\n",
    "             'votes':votes,\n",
    "             'date':date\n",
    "            }\n",
    "    # append the dictionary to the list\n",
    "    rosters.append(dicob)\n",
    "# extra sleep time before moving onto the next meme\n",
    "time.sleep(8)\n",
    "\n",
    "# close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we get the dictionary into a DF, we need to create a list of people we don't want to grab. \n",
    "# This list will include all modmins \n",
    "# Note - this currently doesn't account for people with the same name with different facebook IDs. This can be fixed later\n",
    "\n",
    "ignore = ['Anthony Rivera Straine', 'Frank Straine', 'CrowdSourced Tribune', 'Mateo DeGall', \n",
    "         'Shawn Dixon', 'Jennifer P. Travis', 'Nikolette Adams', 'Lynn Paluga', 'Andrew Harris', \n",
    "          'Ari Rivera Straine', 'C.m. Johnigan']\n",
    "\n",
    "len(ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the roster list to a panda's data frame and drop any duplicate values\n",
    "season = pd.DataFrame(rosters).drop_duplicates()\n",
    "# drop any row where the name = 'Learn More', 'YOUR NAME', or is blank (name = '')\n",
    "season = season[(season['name'] != 'Learn More') & (season['name'] != '') & (~season['name'].isin(ignore))]\n",
    "# reset the index and drop the original index as it's values do not matter\n",
    "season.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a meme contest, anything that is not a meme needs to be ignored. To handle this we will keep any row where the link lenth is not blank (\"\") aka not a picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = season[season['link'] != \"\"]\n",
    "season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to create a maxvote DF to hold the max vote value for each person. Then we merge the maxvote df with the original memecontest df on name and votes. After merging, we use the max function grouped by name to pick the memelink in case of vote ties for the same person. Finally, we assign points to each person where the first 5 index values recieve winner points (5 for 1st, 1 for 5th place). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxvote = season.groupby(['name'])[['votes']].max().reset_index()\n",
    "maxvote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = maxvote.merge(season, how='left', on=['name', 'votes'])\n",
    "grouped = grouped.groupby('name').max().sort_values(by='votes', ascending=False).reset_index()\n",
    "grouped['points'] = 1\n",
    "grouped.at[0,'points'] = 6\n",
    "grouped.at[1,'points'] = 5\n",
    "grouped.at[2,'points'] = 4\n",
    "grouped.at[3,'points'] = 3\n",
    "grouped.at[4,'points'] = 2\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store as CSV\n",
    "\n",
    "This finalizes the memecontest DF (for now). From here we can store the results as a useable CSV\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped.to_csv('Memecontest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your initial csv, each interation can be added to it by concatenating the grouped DF to the loaded CSV and then saving over the loaded CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = pd.read_csv('memecontest.csv')\n",
    "current.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = pd.concat([current, grouped], ignore_index=True)\n",
    "grouped['date'] = pd.to_datetime(grouped['date'])\n",
    "grouped.to_csv('Memecontest.csv')\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped.groupby(by='name')[['name', 'votes', 'points']].sum().sort_values(by='points', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.to_csv('Memecontest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load From CSV\n",
    "\n",
    "Use the cell below if you've already acquired the CSV and want to make changes to it\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### cell for recaputring dataframe from stored csv ###\n",
    "######################################################\n",
    "\n",
    "current = pd.read_csv('Memecontest.csv')\n",
    "current.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Season Winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current.groupby(['name'])[['points']].sum().sort_values(by=[\"points\"], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current['date'] = pd.to_datetime(current['date']).dt.to_period('M')\n",
    "current.to_csv('Memecontest.csv')\n",
    "current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
