{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Intialize](#Initalize)\n",
    "* [Scrapper](#Scraper)\n",
    "* [Clean Data](#Clean-Data)\n",
    "* [Store as CSV](#Store-as-CSV)\n",
    "* [Appendix](#Appendix)\n",
    "  * [Load Data from Roster Dictionary](#Load-From-Roster)\n",
    "  * [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalize\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "from env import muser, mpass\n",
    "print('Init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapper\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code by pythonjar - grants browser permission \n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
    "chrome_options.add_experimental_option(\"prefs\",prefs)\n",
    "\n",
    "#specify the path to chromedriver.exe \n",
    "PATH = 'C:/Program Files (x86)/chromedriver.exe'\n",
    "#creates the driver object with the chrome driver and options as peramaters\n",
    "driver = webdriver.Chrome(PATH, options=chrome_options)\n",
    "\n",
    "#opens FB\n",
    "driver.get(\"http://www.facebook.com\")\n",
    "\n",
    "# waits for FB to load and display the login boxes using CSS_selector types\n",
    "username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='email']\")))\n",
    "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='pass']\")))\n",
    "\n",
    "#clears the user name and password fields and fills them with FB credentials\n",
    "username.clear()\n",
    "username.send_keys(muser)\n",
    "password.clear()\n",
    "password.send_keys(mpass)\n",
    "\n",
    "#waits 5 seconds or until the login button is displayed and then clicks it to login\n",
    "WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click() \n",
    "\n",
    "#wait 30 seconds to get security code for 2way auth\n",
    "time.sleep(30)\n",
    "\n",
    "# wait a random amount of time for the page to load before trying to navigate to the group members page\n",
    "# Note - if facebook limits this behavior, simulating clicks might be necessary\n",
    "time.sleep(random.randrange(4, 11))\n",
    "\n",
    "# create an empty list obj to store the list of dictionaries\n",
    "rosters = []\n",
    "\n",
    "# create a list of group member pages to pull rosters from replace the links inside the [] with the group you want to look at\n",
    "groups = ['https://www.facebook.com/groups/2345147239035182/members',\n",
    "          'https://www.facebook.com/groups/625463614476092/members',\n",
    "          'https://www.facebook.com/groups/756220585201594/members'\n",
    "         ]\n",
    "\n",
    "\n",
    "# loop through the group urls\n",
    "for group in groups:\n",
    "    driver.get(group)\n",
    "    time.sleep(random.randrange(5, 10))\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# use a while true loop to scroll to the end of a group's member list before pulling the page_source\n",
    "    while True:\n",
    "\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page a random amount of time between 6 and 12 seconds\n",
    "        time.sleep(random.randrange(6, 12))\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            # If heights are the same it will exit the function\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "    \n",
    "    html = driver.page_source\n",
    "    # create a soup object using html parser to use to prase the results\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # create a list of all div cards that contains the information for each field we want to dump into a DF\n",
    "    # Note the class values are likely to change, check it yourself.\n",
    "    divs = soup.findAll('div', class_='ow4ym5g4 auili1gw rq0escxv j83agx80 buofh1pr g5gj957u i1fnvgqd oygrvhab cxmmr5t8 hcukyx3x kvgmc6g5 nnctdnn4 hpfvmrgz qt6c0cv9 jb3vyjys l9j0dhe7 du4w35lb bp9cbjyn btwxx1t3 dflh9lhu scb9dxdr')\n",
    "    \n",
    "    # Create a for loop that breaks into each div (data card) and find each instance of each piece of info \n",
    "    for div in divs:\n",
    "        names = div.findAll('a', class_='oajrlxb2 g5ia77u1 qu0x051f esr5mh6w e9989ue4 r7d6kgcz rq0escxv nhd2j8a9 nc684nl6 p7hjln8o kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x jb3vyjys rz4wbd8a qt6c0cv9 a8nywdso i1ao9s8h esuyzwwr f1sip0of lzcic4wl oo9gr5id gpro0wi8 lrazzd5p')\n",
    "        joins = div.findAll('span', class_='d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh e9vueds3 j5wam9gi lrazzd5p m9osqain')\n",
    "        infos = div.findAll('span', class_='d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d9wwppkn fe6kdd0r mau55g9w c8b282yb mdeji52x sq6gx45u a3bd9o3v knj5qynh pipptul6 hzawbc8m')    \n",
    "        groups = div.findAll('span', class_='a8c37x1j ni8dbmo4 stjgntxs l9j0dhe7 ltmttdrg g0qnabr5 ojkyduve')\n",
    "        \n",
    "        # Create empty objects to store the pulled text in the for loops for each piece of data\n",
    "        person = ''\n",
    "        date = ''\n",
    "        add_info = ''\n",
    "        url = ''\n",
    "        title = ''\n",
    "                \n",
    "        # loop through each div obj to pull the required information. This is required because the info is in list form\n",
    "        # some fields are blank hence doing if statements in for loops\n",
    "        for name in names:\n",
    "            person = name.text\n",
    "            url = 'https://www.facebook.com' + str(name.get('href'))\n",
    "        for join in joins:\n",
    "            if len(join.text) > 0:\n",
    "                date = join.text\n",
    "            else:\n",
    "                date = 'none'\n",
    "        for info in infos:\n",
    "            if len(info.text) > 0:\n",
    "                add_info = info.text\n",
    "            else:\n",
    "                add_info = 'none'\n",
    "        for group in groups:\n",
    "            title = group.text\n",
    "        # create a dictionary that contains the information for each member from each div obj\n",
    "        dicob = {'name':person,\n",
    "                 'joined':date,\n",
    "                 'add_info':add_info,\n",
    "                 'user_group_profile':url,\n",
    "                 'group':title}\n",
    "        # append the dictionary to the list\n",
    "        rosters.append(dicob)\n",
    "    # extra sleep time before moving onto the next group\n",
    "    time.sleep(8)\n",
    "\n",
    "# close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the roster list to a panda's data frame and drop any duplicate values\n",
    "recruit = pd.DataFrame(rosters).drop_duplicates()\n",
    "# drop any row where the name = 'Learn More', 'YOUR NAME', or is blank (name = '')\n",
    "recruit = recruit[(recruit['name'] != 'Learn More') & (recruit['name'] != 'Anthony Rivera Straine') & (recruit['name'] != '')]\n",
    "# reset the index and drop the original index as it's values do not matter\n",
    "recruit.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print the head of the DF\n",
    "recruit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to split the link into links that go to the member's profile, the member's group profile, and the group link. After processing the links, we will add them to our original recurit DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe by splitting the link text into section from /\n",
    "links = recruit['user_group_profile'].str.split('/', expand=True)\n",
    "links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this instance, columns 1 and 7 are worthless and can be dropped\n",
    "links.drop(columns=[1, 7], inplace=True)\n",
    "# crate the member FB profile link. The 6th column is the FB user's unique ID.\n",
    "# The ID can be used to access a their profile by appending it to \"https://www.facebook.com/profile.php?id=\"\n",
    "links['profile_link'] = \"https://www.facebook.com/profile.php?id=\" + links[6]\n",
    "# Column 4 is the group's unique ID. A group can be accessed by appending the group id to \"https://www.facebook.com/groups/\"\n",
    "links['group_link'] = \"https://www.facebook.com/groups/\" + links[4]\n",
    "# The rest \n",
    "links = links[['profile_link', 'group_link']]\n",
    "links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then concatenate the recurit DF with the links DF to get the processed links into the DF\n",
    "recruit = pd.concat([recruit, links], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the useful links processed from the data, we can moveon to processing the Joined column. The joined column contains both an estimated join date and how they joined the group. For now, we will only process out an estaimated time_in_group column which will be a catagory variable of ***Less Than a Year*** or ***A Year or More***. We will also crate an acquired date column which will be the date the script was run for posterity.\n",
    "\n",
    "---\n",
    "**NOTE** The current code does not account for admins in groups or blank joined field. This will be remedied at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a acquired_date column\n",
    "recruit['aquired_date'] = pd.to_datetime('now').strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# To create the time in group variable we will use np.where to find instances where joined does not (~) contains 'year'\n",
    "# If year is present it will be A year or more and if not it will be a year or less. \n",
    "recruit['time_in_group'] = np.where(~recruit['joined'].str.contains('year'), 'Less Than a Year', 'A Year or More')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store as CSV\n",
    "\n",
    "This finalizes the recurit DF (for now). From here we can store the results as a useable CSV\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recuritment = pd.read_csv('Recruitment.csv')\n",
    "print(recruit.shape[0])\n",
    "print(recuritment.shape[0])\n",
    "recruit = pd.concat([recuritment, recruit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruit.to_csv('Recruitment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Roster\n",
    "\n",
    "Use the cell below if you've already acquired the roster List and want to turn it into a CSV\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "### cell for recaputring dataframe from stored rosters ###\n",
    "##########################################################\n",
    "\n",
    "# convert the roster list to a panda's data frame and drop any duplicate whole row values\n",
    "recruit = pd.DataFrame(rosters).drop_duplicates()\n",
    "# drop any row where the name = 'Learn More', 'your name', or is blank (name = '')\n",
    "recruit = recruit[(recruit['name'] != 'Learn More') & (recruit['name'] != 'Anthony Rivera Straine') & (recruit['name'] != '')]\n",
    "# reset the index and drop the original index as it's values do not matter\n",
    "recruit.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "recruit.to_csv(\"Recruitment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load From CSV\n",
    "\n",
    "Use the cell below if you've already acquired the CSV and want to make changes to it\n",
    "\n",
    "**Jump-to**: [Table of Content](#Table-of-Contents) | [Scrapper](#Scrapper) | [Clean Data](#Clean-Data) | [Store as CSV](#Store-as-CSV) | [Load Data from Roster Dictionary](#Load-From-Roster) | [Load Data from CSV](#Load-From-CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### cell for recaputring dataframe from stored csv ###\n",
    "######################################################\n",
    "\n",
    "recruit = pd.read_csv('Recruitment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
